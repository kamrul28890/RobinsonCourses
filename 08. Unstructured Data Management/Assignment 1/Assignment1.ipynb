{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de221561-b68c-4ce5-88b5-5d544a3c380b",
   "metadata": {},
   "source": [
    "# Assignment 1 - Text Preprocessing and Similarity\n",
    "This assignment will focus on text preprocessing and the calculation of text similarity through TF-IDF. Please put the data file `kickstarter_desc_sample.csv` under the same directory with this notebook. \n",
    "\n",
    "The code below will load the data and select a random sample for subsequent tasks. Each student will work on a different random sample. Then it will display how the data looks like in your sample. \n",
    "\n",
    "The code in the notebook primarily uses `pandas` dataframe. You can use `df['column'].apply(function)` to apply any preprocessing function directly on pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5851729-5723-46f3-90a4-beca75aee972",
   "metadata": {},
   "source": [
    "The data was collected from the a crowdfunding website, Kickstarter (<a>www.kickstarter.com</a>). It is basically about entrepreneurial fundraising campaigns. The sample contains around five thousand project descriptions. For each row in the data, there is a `project_id`, the unique identifier of the project, and a column of textual project description (`description_str`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074730ef-cbe1-4969-8adb-af04179335ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "descriptions = pd.read_csv('./kickstarter_desc_sample.csv')\n",
    "descriptions = descriptions.sample(1000)\n",
    "import platform\n",
    "platform.uname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa49400-1ed7-46f9-925e-a63a89b701dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "descriptions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c1882-7e4b-4349-ab96-052af5936bb7",
   "metadata": {},
   "source": [
    "## Task 1 Text Preprocessing\n",
    "In Task 1 you will preprocess the text. The text has lots of noises and unstructured terms so before performing subsequent analysis, you need to clean them first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a7f5d0-f557-41bd-9583-7cb64dfcd3fa",
   "metadata": {},
   "source": [
    "In this task, you will implement (1) `tokenizer` (2) `stop words removal` (3) `lemmatizer` to preprocess the texts in the column `description_str`. Eventually, the cleaned text should be saved as a separate column in the `descriptions` dataframe with the name `description_clean`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacfb9aa-e6c2-4340-bb85-d1ff02c91112",
   "metadata": {},
   "source": [
    "First make sure you download the NLTK corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01611441-e67f-410c-8355-63f57eb37140",
   "metadata": {},
   "source": [
    "### `Task 1.1` Tokenizer\n",
    "Implement tokenizer to tokenize the text and remove special characters in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026d69d8-98b6-4137-871c-ebdc95b6331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def tokenizer(description):\n",
    "    #tokenize to extract words\n",
    "    description = description.lower()\n",
    "    description = re.sub('[^a-zA-Z0-9]', ' ', description)\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    words = tokenizer.tokenize(description)\n",
    "    tokenized_words = ' '.join(words)\n",
    "    return tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00893a02-c87f-4cef-9b2d-ac7f8dc296b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions['description_clean'] = descriptions['description_str'].apply(tokenizer)\n",
    "descriptions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443294f8-7848-425b-a82a-ae1f6cdcc10a",
   "metadata": {},
   "source": [
    "### `Task 1.2` Stopwords\n",
    "Implement stop words removal to remove english stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff7419-b3c7-4216-968f-7f0222a61a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    #Your code to implement stopwords removal process in column description_clean\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d19563-36f8-46ca-bd32-9ff9a30b425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions['description_clean'] = descriptions['description_clean'].apply(remove_stopwords)\n",
    "descriptions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84702113-b6e3-4dad-83f6-dda28cce6059",
   "metadata": {},
   "source": [
    "### `Task 1.3` Lemmatizer\n",
    "Implement lemmatizer on the tokens in the text using WordNet lemmatizer with POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee4c6f3-1f06-4415-b4e4-f3f320612b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_part_of_speech_tags(token):\n",
    "    \n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    \n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    \n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e3fa77-691c-4867-931e-3d6dad39e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[valid]\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def postag_lemmentization(words):\n",
    "    #Your code to implement WordNet lemmatizer with POS tags in column description_clean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ded28-c12d-406e-8325-5beba7d7645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions['description_clean'] = descriptions['description_clean'].apply(postag_lemmentization)\n",
    "descriptions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc215bd-ca43-419c-99fb-9451d6bb14a6",
   "metadata": {},
   "source": [
    "The following code will do some further cleaning by removing all the numbers in the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adc6644-0513-4a47-afec-9b2d900fc6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions['description_clean'] = descriptions['description_clean'].str.replace('\\d+', '')\n",
    "descriptions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772a63f-6e1a-4ad4-ac1e-db2832b86c7f",
   "metadata": {},
   "source": [
    "## Task 2 Use TF-IDF to represent text and calculate similarity\n",
    "In the second task, you will calculate cosine similarity from TF-IDF representation of document. \n",
    "\n",
    "In the following code, the `descriptions` dataframe will be randomly split into one main sample and one small test sample (3 project descriptions). \n",
    "\n",
    "Then what you are going to do are: \n",
    "\n",
    "(1) build TF-IDF vector representation from `desc_main` sample with `description_clean` column and show the shape of TF-IDF matrix; \n",
    "\n",
    "(2) for each of the three project descriptions in the `desc_test`, find the most similar project from `desc_main` sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1a074-0e7b-4c7e-ad98-2a333c9a4234",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_test = descriptions.sample(3)\n",
    "desc_main = descriptions.drop(desc_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7803c0-2482-4bb1-9fde-b124f0db2d3f",
   "metadata": {},
   "source": [
    "### `Task 2.1` TF-IDF\n",
    "Build TF-IDF vector representation for the project descriptions in `desc_main` and show the `shape` of TF-IDF matrix. When building TF-IDF matrix, only use up to top `500` terms. Use CountVectorizer and TF-IDF transformer to build the matrices. Consider `9.Chatbot.ipynb` as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9effb17-9d0a-4f53-b7b3-45bea71e1b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code to build TF-IDF vectorizer from desc_main['description_clean']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2e8f7-b440-432f-9546-53a7c9888506",
   "metadata": {},
   "source": [
    "### `Task 2.2` Search Similar Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5caed-c264-4ac1-a58c-6e76562cb3f2",
   "metadata": {},
   "source": [
    "Now you need to find the project_ids and descriptions in `desc_test`. The code below shows the three projects in that dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f5d70-912b-49bb-ab9e-9ef8530f7040",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6167cb2-a6c8-407c-9764-8fb9de35ecc2",
   "metadata": {},
   "source": [
    "For these three projects in `desc_test`, find the most similar project for each of them in `desc_main`. Print the `project_id` of the most similar project description and the corresponding `cosine similarity`. \n",
    "\n",
    "Edit the headings below to replace `project_id` with the three `project_id`s you get and then implement your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef5362-bfa9-409f-a44c-0b2273fb9d3e",
   "metadata": {},
   "source": [
    "#### Get the most similar project for `project_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844224dc-b924-45ba-baf5-4df6c94a08c9",
   "metadata": {},
   "source": [
    "Using this line of code to get project description with a given project_id: \n",
    "\n",
    "`project_desc = desc_test[desc_test['project_id']==1617375102]['description_clean'].values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac971b68-0dbc-45d6-8fcf-e9a80b4729ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code to get the most similar project for project 1 from the TF-IDF vectorizer\n",
    "\n",
    "\n",
    "#Print the similarity score and corresponding project_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c71ced-0328-47d1-a277-1475a62bec08",
   "metadata": {},
   "source": [
    "#### Get the most similar project for `project_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f8b14-5316-4788-ae6b-115151d33e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code to get the most similar project for project 2 from the TF-IDF vectorizer \n",
    "\n",
    "\n",
    "#Print the similarity score and corresponding project_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a398a0-76ea-4533-bb32-a190b7688b4a",
   "metadata": {},
   "source": [
    "#### Get the most similar project for `project_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95304ea7-c017-4899-ab1b-ded9133612d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code to get the most similar project for project 3 from the TF-IDF vectorizer \n",
    "\n",
    "\n",
    "#Print the similarity score and corresponding project_id\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
